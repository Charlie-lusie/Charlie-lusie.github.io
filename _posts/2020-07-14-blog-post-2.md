---
title: 'Embedding Feature理解'
date: 20120-07-14
permalink: /posts/2020/07/blog-post-2/
tags:
  - Machine Learning
  - Paper Reading
---

embedding可能首先出现在NLP领域。网上搜索embedding，结果中有很多word embeddings相关的内容。接着embedding逐渐扩散成为机器学习中的一个经典概念。

embedding在数学2上指：一个数学结构经映射包含到另一个结构中。直接引用wikipedia的原文：

> When some object X is said to be embedded in another object Y, the embedding is given by some injective and structure-preserving map f : X → Y.

embedding就是映射的过程：X映射到Y，这个映射由两个特点：injective和structure-preserving。injective就是单射(回想一下函数的概念)，指所有x,y∈A,且x≠y，都有f(x)≠f(y)；而structure-preserving可以翻译为保持结构，这里“保持结构”的具体含义，需要根据所讨论的结构或具体数学问题而定，比如在X所在的空间，有X1<X2, 那么映射得到的Y1，Y2在Y空间也有约束：Y1<Y2。

所以在机器学习领域，embedding指将数据转换为合适的特征表示，合适是指样本特定的属性可以用距离的概念表示出来。通过embedding可以将原始样本数据转换为适合机器学习的特征数据。比如，针对同一个语音样本集，一个区分说话人的模型可以把语音样本转换为一个数值向量，来自同一人的语音样本对应的embedding features有着更小的欧式距离；然而对于另一个识别语音内容的模型，说了相同话的语音(而不是同一个人的语音)对应的embedding features会有更小的欧式距离。甚至你还可以再设计一种映射来表达“愤怒”，“高兴”等情绪。embedding features就是方便用距离表示属性相似程度的特征，属性相似的样本对应的特征之间距离更小，这正是前一段提到的structure-preserving。

之前提到过，embedding在NLP领域出现更多些。我们就拿NLP领域的一个神器word2vec举个例子，word2vec可以把单词转换成一个很神奇的特征空间中的点(向量): 假设Φ(X)是X对应的embedding features, 那么Φ(Germany) - Φ(Berlin) ≈ Φ(France) - Φ(Paris)! 首都这个概念也能用embedding features表示！

相应的，CV领域的embedding也表示类似的概念，手绘数字图像对应的稀疏矢量特征首先用一个预先设计的映射转换为embedding features后再作为神经网络的输入。embedding features是更好的特征表达。
