---
title: '[CVPR 2020] Joint 3D Instance Segmentation and Object Detection for Autonomous Driving'
date: 20120-07-14
permalink: /posts/2020/07/blog-post-1/
tags:
  - Detection
  - LiDAR
  - Paper Reading
---
这篇文章的思路是讲分割与检测结合起来，用分割来提升检测的精度，并且能免除掉NMS等操作；

贡献
------

- 提出spatial embedding（SE）特征，使用到了bbox信息和点特征
- 提出一个统一的网络框架来同时完成检测和实例分割

做法
------

第一阶段网络：首先使用一个backbone提取点级别特征，比如PointNet++，然后分为语义分割和实例相关SE两支。在SE的结果基础上，使用一个聚类层来产生实例分割结果并且生成对应的bbox proposal。

然后是第二阶段的网络，用一个refine网络对上面产生的proposal进行细化调整。

从第一阶段说起。
- 点特征提取用的是pointnet++，当然也可以换别的，但我觉得目前就pn++比较好，能兼顾局部和全局特征。
- 语义分割所需要的信息就来自上面pn++提取到的合并特征，并且为了应对类间不平衡，使用了focal loss：
$$
L_{cls} = -\sum_{i=1}^C (y_i\log (p_i))(1-p_i)^\gamma \alpha_i + (1- y_i)\log (1-p_i) p_i^\gamma (1 - \alpha_i))
$$

其中C是类的数量，yi=1，如果真值为第i类，否则为0；pi就是预测的第i类的得分。
- 这个所谓的SE特征实质上是取每个前景点，计算其与对应物体中心点的距离，回归的目标就是这个offset：
$$
Offset^I = (p_x^i-c_x^k, p_y^i-c_y^k, p_z^i-c_z^k)^T
$$

有了这个SE特征，那所有的前景点都汇集到对应中心了。因此区分不同实例的前景点只需要一个简单的聚类即可。

第二阶段的bbox refinement搞了一个instance aware ROI pooling策略，这个策略有两件事需要注意：首先属于同一聚类的前景点都会被用于refinement，即使有的点不在box proposal内部，其次有的bbox内部的点，如果对应聚类标签不是这个障碍物，则弃之不用。然后用这些点以及上一阶段提取到的特征再进行特征提取，然后网络头输出分类和box信息。
